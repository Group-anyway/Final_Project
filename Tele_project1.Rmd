---
title: "HW6 Telemarketing"
author: "Anyway team"
date: "3/22/2020"
output:
  html_document:
    toc: true
    theme: readable
    highlight: tango
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## Downloading and Prepping the Data

```{r}
#Downloading and Prepping the Data
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
summary(tele)

#We are deleting the "duration" variable because it is an after the fact measurement. We only should be using variables that we know before the call
tele$duration <- NULL

# Deleting the column X
tele$X <- NULL

# Changing pdays to a dummy and deleting pdays
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)
tele$pdays <- NULL

str(tele)
```

## Getting Data Ready for Analysis

```{r}
# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric

telemm <- as.data.frame(model.matrix(~.-1,tele))
str(telemm)

# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
tele_random <- telemm[sample(nrow(telemm)),]

#Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# we are going to normalize everything 
tele_norm <- as.data.frame(lapply(tele_random, normalize))
```


## Getting Train and Test Samples

```{r}
# Selects 10000 random rows for test data
set.seed(12345)
test_set <- sample(1:nrow(tele_norm), 10000) 
# Depending on R-version and computer, different rows may be selected. 
# If that happens, results are different. 

# Create a train set and test set
#First the predictors - all columns except the yyes column
tele_train <- tele_norm[-test_set, -match("yyes",names(tele_norm))]
tele_test <- tele_norm[test_set, -match("yyes",names(tele_norm))]

#Now the response (aka Labels) - only the yyes column
tele_train_labels <- tele_norm[-test_set, "yyes"]
tele_test_labels <- tele_norm[test_set, "yyes"]

```

> Now you are ready to build your ANN model. Feel free to modify the data load, cleaning and preparation code above as per your preference.


## library
```{r}
library(caret)
library(class)
```


### knn
```{r}
knn_pred <- knn(train = tele_train, test = tele_test, cl = tele_train_labels, k = 15)
summary(knn_pred)
confusionMatrix(knn_pred, as.factor(tele_test_labels), positive = "1")
```


### logisticregression  make prediction
```{r}
simplemodel <- glm(tele_train_labels ~ ., data = tele_train, family = "binomial")
summary(simplemodel)
testpred <- predict(simplemodel, newdata = tele_test, type = "response")
summary(testpred)
binpred <- ifelse(testpred >= 0.3, 1, 0)

```


### logistic regression confusionmatrix_logisitic
```{r}
confusionMatrix(as.factor(binpred), as.factor(tele_test_labels), positive = "1")
```


### decision trees model
```{r}
library(C50)
library(caret)
```

###DT build model
```{r}
telemodel <- C5.0(as.factor(tele_train_labels) ~., data = tele_train)
plot(telemodel)
```

### DT predict&confusionmatrix
```{r}
dtpred <- predict(telemodel, tele_test)
summary(dtpred)
confusionMatrix(as.factor(dtpred), as.factor(tele_test_labels), positive = "1")
```

### Ann load library
```{r}
library(neuralnet)
library(caret)
```

##ANN build model
```{r}
simplemodel1 <- neuralnet(tele_train_labels ~ ., data = tele_train, hidden = 5)
plot(simplemodel1)
```

##ANN predict model
```{r}
annpred <- predict(simplemodel1, tele_test)
summary(annpred)
annbinpred <- ifelse(annpred >= 0.5, 1, 0)
summary(annbinpred)
```
### ANN confusionmatrix
```{r}
confusionMatrix(as.factor(annbinpred), as.factor(tele_test_labels), positive = "1")
```


### combine everything
```{r}
knn_pred_numeric <- as.numeric(as.character(knn_pred))
annbinpred_numeric <- as.numeric(as.character(annbinpred))
binpred_numeric <- as.numeric(as.character(binpred))
dtpred_numeric <- as.numeric(as.character(dtpred))

combined_pred <- pmax(knn_pred_numeric, annbinpred_numeric, binpred_numeric, dtpred_numeric)

confusionMatrix(as.factor(combined_pred), as.factor(tele_test_labels), positive = "1")
```

## Conclusion
### After analyzing the data using four individual models and a combined model, we found the following accuracies: 89.51% for KNN, 88.95% for logistic regression, 89.92% for the decision tree, and 89.74% for the ANN. Additionally, the combined model achieved an accuracy of 88.86%.

### Based on these findings, the decision tree model emerged as the most promising choice with the highest accuracy among the individual models, reaching 89.92%. Its ability to capture complex decision boundaries makes it suitable for our prediction task. Therefore, I would recommend utilizing the decision tree model for improving the success of TeleMarketing efforts.

### However, since it's a practical business issue, we would like to compare the cost-revenue of each model. For the combined model, the cost of misprediction is 499+615*6=4189. For the KNN model, the cost of misprediction is 117+933*6=5715. For Logistic regression, the cost of misprediction is 455+650*6=4355. For ANN model, the cost of misprediction is 173+853*6=5291. For decision tree, the cost of misprediction is 129+879*6=5403. From this perspective, combined model stands out as well since it generates the smallest cost of misprediction. 

