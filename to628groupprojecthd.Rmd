---
title: "to628groupproject"
author: "Group Anyway"
date: "2024-04-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## load library
```{r}
library(janitor)
library(caret)
library(class)
library(neuralnet)
library(C50)
library(randomForest)
library(kernlab)
library(caret)
hd <- read.csv("framingham.csv")
summary(hd)
table(hd$TenYearCHD) ##the data of depende variable are imbalacing, since we have 644 for "1", and 3594 for "0". Thus we will address the imbalaced dataset by setting a smaller threshold when generating the binary prediction, and picking proper parameters for cost matrix. 
```

## Getting Data Ready for Analysis
```{r}
hdmm <- as.data.frame(model.matrix(~.-1,hd))
hdmm <- clean_names(hdmm)


train_ratio <- 0.5 
set.seed(12345)
train_rows <- sample(1: nrow(hdmm), train_ratio*nrow(hdmm))

hd_train_lr_dt_rf <- hdmm[train_rows, ]
hd_test_lr_dt_rf <- hdmm[-train_rows, ]

## data for ann
normalize <- function(x) {
   ((x - min(x)) / (max(x) - min(x)))
}

hd_norm <- as.data.frame(lapply(hdmm, normalize))
hd_train_ann <- hd_norm[train_rows, ]
hd_test_ann <- hd_norm[-train_rows, ]

## data for knn
## separating labels (y values) and predictors (x values)
hd_train_knn_x <- hd_norm[-train_rows, -match("ten_year_chd",names(hd_norm))]
hd_test_knn_x <- hd_norm[train_rows, -match("ten_year_chd",names(hd_norm))]

hd_train_knn_y <- hd_norm[-train_rows, "ten_year_chd"]
hd_test_knn_y <- hd_norm[train_rows, "ten_year_chd"]

```

## logistic regression
```{r}
simplemodel <- glm(hd_train_lr_dt_rf$ten_year_chd ~., data = hd_train_lr_dt_rf, family = "binomial")
summary(simplemodel)
lrpred <- predict(simplemodel, newdata = hd_test_lr_dt_rf, type = "response")
lrbinpred <- ifelse(lrpred >= 0.3, 1, 0) ##Because of the imbalanced dataset, we need to set a small threshold to guarantee the confusion matrix can better capture the minority part. 


summary(lrbinpred)
confusionMatrix(as.factor(lrbinpred), as.factor(hd_test_lr_dt_rf$ten_year_chd), positive = "1")
```

## knn model
```{r}
knn_pred <- knn(train = hd_train_knn_x, test = hd_test_knn_x, cl = hd_train_knn_y, k = 5, prob = TRUE)
summary(knn_pred)
## Since the imbalaced dataset, we need to pick a smaller k. A smaller k enables the model to capture more of the local structure of the data, which can be critical if the minority class forms compact clusters surrounded by the majority class. A larger k might overlook these local patterns in favor of broader, global patterns that tend to favor the majority class.


knn_raw_pred <- attributes(knn_pred)$prob ## we use probaility, the raw data, instead of the binary number. 

confusionMatrix(as.factor(knn_pred), as.factor(hd_test_knn_y), positive = "1")
```

## svm model
```{r}
hd_svm1 <- ksvm(ten_year_chd ~ ., data = hd_train_lr_dt_rf, kernel = "vanilladot")
hd_svm2 <- ksvm(ten_year_chd ~ ., data = hd_train_lr_dt_rf, kernel = "rbfdot")
hd_svmpred1 <- predict(hd_svm1, hd_test_lr_dt_rf)
hd_svmpred2 <- predict(hd_svm2, hd_test_lr_dt_rf)
hd_svmpredbin <- ifelse(hd_svmpred2 >= 0.05, 1, 0) ## imbalance, 对比一下和别的数字0.5 or 0.2
 
#if we set the threshold to 0.5, we will get True Negatives (TN): 1561; False Positives (FP): 4; False Negatives (FN): 259; 
True Positives (TP): 4 
# if we set the threshold to 0.2, we will get True Negatives (TN): 1288; False Positives (FP): 277; False Negatives (FN): 157; True Positives (TP):106
# After we compare two matrix, we can found : 
#1)Poor F1 Score for the Minority Class: The F1 Score, which balances precision and recall, may be quite low for the minority class. The model's precision is TP / (TP + FP), which is also low (4 / (4 + 4) = 0.5 in this case), and the F1 Score will be affected by both.
#2)Sensitivity (Recall) is Extremely Low (0.015209 ): Since Sensitivity is TP / (TP + FN), and TP is 4 in the larger threshold. 
#3)Specificity is High(0.997444), but Misleading: Specificity is TN / (TN + FP), which will be high due to the high TN. This could mistakenly suggest that the model is performing better than it actually is.

confusionMatrix(as.factor(hd_svmpredbin), as.factor(hd_test_lr_dt_rf$ten_year_chd), positive = "1")
```
## decision tree
```{r}
summary(hd_test_lr_dt_rf)

library(rpart)

dtmodel <- rpart(ten_year_chd ~ ., data = hd_train_lr_dt_rf)
dtpred <- predict(dtmodel, hd_test_lr_dt_rf)

summary(dtpred)
dtpredbin <- ifelse(dtpred >= 0.2, 1, 0) ##同上

confusionMatrix(as.factor(dtpredbin), as.factor(hd_test_lr_dt_rf$ten_year_chd), positive = "1")
```

## random forest
```{r}
set.seed(12345)
rfmodel <- randomForest(ten_year_chd ~., data = hd_train_lr_dt_rf)
summary(rfmodel)
rfpred <- predict(rfmodel, hd_test_lr_dt_rf)
rfpredbin <- ifelse(rfpred >= 0.2, 1, 0)
summary(rfpred)

confusionMatrix(as.factor(rfpredbin), as.factor(hd_test_lr_dt_rf$ten_year_chd), positive = "1")
```
## ann model
```{r}
annmodel <- neuralnet(ten_year_chd ~ ., data = hd_train_ann, hidden = 5)
ann_pred <- predict(annmodel, newdata = hd_test_ann)
ann_pred_bin <- ifelse(ann_pred >= 0.5, 1, 0)
confusionMatrix(as.factor(ann_pred_bin), as.factor(hd_test_ann$ten_year_chd), positive = "1")
```

## combine everything
```{r}
hd_combined <- data.frame(lrpred, knn_raw_pred, ann_pred, hd_svmpred1, hd_svmpred2, dtpred, rfpred, hd_test_lr_dt_rf$ten_year_chd)

trainratio <- 0.7
set.seed(12345)
train_rows <- sample(1:nrow(hd_combined), trainratio*nrow(hd_combined))

train_2 <- hd_combined[train_rows, ]
test_2 <- hd_combined[-train_rows, ]

model_2 <- C5.0(as.factor(hd_test_lr_dt_rf.ten_year_chd) ~. , data= train_2)
pred_2 <- predict(model_2, test_2)
confusionMatrix(as.factor(pred_2), as.factor(test_2$hd_test_lr_dt_rf.ten_year_chd), positive = "1")
```
## error cost matrix
```{r}
cost_matrix <- matrix(c(0,1,3,0), nrow =2 ) ## we increase the cost for false positive. 

cost_model <- C5.0(as.factor(hd_test_lr_dt_rf.ten_year_chd) ~. , data= train_2, costs = cost_matrix)
plot(cost_model)
pred_cost <- predict(cost_model, test_2)
confusionMatrix(as.factor(pred_cost), as.factor(test_2$hd_test_lr_dt_rf.ten_year_chd), positive = "1")
```

